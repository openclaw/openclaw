---
name: governed-agents
description: Accountable sub-agent orchestration with task contracts, verification gates, and reputation tracking. Use when spawning Codex or other sub-agents to implement features, fixes, or tasks â€” ensures agents can't fake success and builds a persistent reputation score per model. Triggers on requests involving sessions_spawn, sub-agent task delegation, or when you need verifiable outcomes from AI sub-agents.
homepage: https://github.com/Nefas11/openclaw-superpowers-workflow
metadata:
  {
    "openclaw":
      {
        "emoji": "ðŸ›¡ï¸",
        "requires": { "bins": ["python3"], "python": [">=3.10"] },
      },
  }
---

# Governed Agents

A lightweight system that brings **accountability** to OpenClaw sub-agent orchestration via:

1. **Task Contracts** â€” define objective + acceptance criteria BEFORE spawning
2. **Verification Gates** â€” independently check deliverables after completion (Files â†’ Tests â†’ Lint â†’ AST)
3. **Reputation Ledger** â€” persistent per-model score; hallucinated success â†’ âˆ’1.0

## Setup

Run once to install the package into your workspace:

```bash
cd ~/.openclaw/workspace
git clone https://github.com/Nefas11/openclaw-superpowers-workflow governed_agents_src
cp -r governed_agents_src/governed_agents ./governed_agents
rm -rf governed_agents_src
```

No pip dependencies â€” uses only Python stdlib (sqlite3, subprocess, ast, glob).

## Core Workflow

### 1. Create a contract + spawn

```python
from governed_agents.orchestrator import GovernedOrchestrator

g = GovernedOrchestrator.for_task(
    objective="Add JWT authentication to API",
    model="openai/gpt-5.2-codex",
    criteria=[
        "POST /api/auth/login returns JWT token",
        "Invalid credentials return 401",
        "Tests pass: pytest tests/test_auth.py",
    ],
    required_files=["api/auth.py", "tests/test_auth.py"],
    run_tests="pytest tests/test_auth.py -v",
)

# Use g.instructions() as the task prompt for sessions_spawn
result = sessions_spawn(task=g.instructions(), model="Codex")
```

### 2. Record outcome after completion

```python
# After sub-agent completes:
verification = g.record_success()
# â†’ Verifier automatically checks required_files + runs tests
# â†’ If verification FAILS: score = -1.0, status = "failed" (hallucinated success)
# â†’ If verification PASSES: score = +1.0, status = "success"

# For honest blockers (sub-agent couldn't proceed):
g.record_blocked("Missing API key for external service")
# â†’ score = +0.5 (rewarded for honesty, no verification)

# For failures:
g.record_failure("Task not attempted within timeout")
# â†’ score = 0.0
```

## Verification Gates

Gates run sequentially â€” first failure stops the chain:

| Gate | Configured by | Pass condition | Skip when |
|------|--------------|---------------|-----------|
| **Files** | `required_files=["path/to/file.py"]` | All files/globs exist | list empty |
| **Tests** | `run_tests="pytest tests/ -v"` | Exit code 0 | not set |
| **Lint** | `run_lint=True, lint_paths=["src/"]` | Exit code 0 | graceful skip if no linter installed |
| **AST** | `check_syntax=True` (default) | All .py files parse without SyntaxError | `check_syntax=False` |

```python
# Full contract example with all gates:
g = GovernedOrchestrator.for_task(
    objective="Refactor payment module",
    model="openai/gpt-5.2-codex",
    criteria=["No regressions", "Lint clean"],
    required_files=["app/payment.py", "tests/test_payment.py"],
    run_tests="pytest tests/test_payment.py -v",
    run_lint=True,
    lint_paths=["app/payment.py"],
    check_syntax=True,   # default
)
```

## Score Matrix

| Outcome | Score | Condition |
|---------|-------|-----------|
| Verified success | **+1.0** | `record_success()` + all gates pass |
| Hallucinated success | **âˆ’1.0** | `record_success()` + any gate fails |
| Honest blocker | **+0.5** | `record_blocked("reason")` |
| Failure | **0.0** | `record_failure("reason")` |

## Reputation & Supervision

```python
from governed_agents.reputation import get_agent_stats

stats = get_agent_stats()
for agent in stats:
    print(f"{agent['agent_id']}: {agent['reputation']:.3f} ({agent['supervision']['level']})")
```

| Reputation | Supervision Level |
|-----------|-----------------|
| > 0.8 | Autonomous |
| 0.6 â€“ 0.8 | Standard |
| 0.4 â€“ 0.6 | Supervised |
| < 0.4 | Strict |

## Task History (Dashboard)

If you have the OpenClaw Command Center running, add the governed agents widget to `app.py`:

```python
@app.get("/api/governed/latest")
async def governed_latest(_=Depends(verify_token)):
    from governed_agents.reputation import get_agent_stats, get_task_history
    return {
        "agents": get_agent_stats(),
        "recent_tasks": get_task_history(limit=10)
    }
```

## Anti-Patterns

âŒ Spawning without a contract â†’ no verification, no accountability  
âŒ Skipping `record_success()` â†’ reputation never updated  
âŒ Empty `required_files` â†’ Files gate skipped, agent can claim anything  
âŒ Using `record_success()` before checking the sub-agent actually finished  

## File Structure

```
governed_agents/
â”œâ”€â”€ contract.py        # TaskContract dataclass
â”œâ”€â”€ orchestrator.py    # GovernedOrchestrator (for_task, record_*)
â”œâ”€â”€ reputation.py      # SQLite DB, scoring, get_agent_stats()
â”œâ”€â”€ verifier.py        # 4-gate verification pipeline
â”œâ”€â”€ self_report.py     # CLI for sub-agents to self-report
â””â”€â”€ test_verification.py  # Unit tests (run to verify install)
```

Verify installation:
```bash
cd ~/.openclaw/workspace
python3 governed_agents/test_verification.py
# Expected: ðŸ† ALL VERIFICATION GATE TESTS PASS
```
