/**
 * Unified Voice Provider Interface
 *
 * Common interface for real-time voice providers:
 * - OpenAI Realtime API
 * - Google Gemini Live API
 *
 * Both support native voice-to-voice with low latency.
 */

import { EventEmitter } from "node:events";

// ============================================
// Provider Types
// ============================================

export type VoiceProviderType = "openai" | "gemini";

export interface VoiceProviderConfig {
  provider: VoiceProviderType;
  apiKey: string;
  /** Model to use */
  model?: string;
  /** Voice for TTS output */
  voice?: string;
  /** System instructions */
  instructions?: string;
  /** Language code (e.g., 'ko', 'en') */
  language?: string;
  /** Enable voice activity detection */
  enableVAD?: boolean;
  /** VAD sensitivity threshold (0.0 - 1.0) */
  vadThreshold?: number;
  /** Silence duration to end turn (ms) */
  silenceDurationMs?: number;
  /** Maximum session duration (ms) */
  maxDurationMs?: number;
  /** Custom endpoint URL (for proxies) */
  endpoint?: string;
}

// ============================================
// Audio Configuration
// ============================================

export interface AudioConfig {
  /** Sample rate in Hz */
  sampleRate: number;
  /** Bits per sample */
  bitsPerSample: 16 | 32;
  /** Number of channels */
  channels: 1 | 2;
  /** Audio codec */
  codec: "pcm" | "opus" | "g711_ulaw" | "g711_alaw";
}

/** OpenAI Realtime API audio config */
export const OPENAI_AUDIO_CONFIG: { input: AudioConfig; output: AudioConfig } = {
  input: { sampleRate: 24000, bitsPerSample: 16, channels: 1, codec: "pcm" },
  output: { sampleRate: 24000, bitsPerSample: 16, channels: 1, codec: "pcm" },
};

/** Gemini Live API audio config */
export const GEMINI_AUDIO_CONFIG: { input: AudioConfig; output: AudioConfig } = {
  input: { sampleRate: 16000, bitsPerSample: 16, channels: 1, codec: "pcm" },
  output: { sampleRate: 24000, bitsPerSample: 16, channels: 1, codec: "pcm" },
};

// ============================================
// Session & Events
// ============================================

export type SessionStatus =
  | "idle"
  | "connecting"
  | "connected"
  | "listening"
  | "thinking"
  | "speaking"
  | "interrupted"
  | "error"
  | "closed";

export interface VoiceSession {
  id: string;
  provider: VoiceProviderType;
  userId: string;
  status: SessionStatus;
  createdAt: Date;
  lastActivity: Date;
  turnCount: number;
  durationMs: number;
  stats: SessionStats;
}

export interface SessionStats {
  inputAudioBytes: number;
  outputAudioBytes: number;
  inputTokens: number;
  outputTokens: number;
  latencyMs: number[];
  interruptions: number;
}

export interface VoiceProviderEvents {
  // Session lifecycle
  "session.created": (session: VoiceSession) => void;
  "session.connected": (session: VoiceSession) => void;
  "session.error": (error: Error, session: VoiceSession) => void;
  "session.closed": (reason: string, session: VoiceSession) => void;

  // User input
  "input.started": () => void;
  "input.audio": (chunk: Buffer) => void;
  "input.ended": () => void;
  "input.transcript": (text: string, isFinal: boolean) => void;

  // AI response
  "response.started": () => void;
  "response.audio": (chunk: Buffer) => void;
  "response.text": (text: string, isFinal: boolean) => void;
  "response.ended": () => void;
  "response.interrupted": () => void;

  // Tool/Function calls
  "tool.call": (name: string, args: unknown) => void;
  "tool.result": (name: string, result: unknown) => void;

  // Audio level (for visualizations)
  "audio.level": (level: number, direction: "input" | "output") => void;
}

// ============================================
// Tool Definition
// ============================================

export interface VoiceTool {
  name: string;
  description: string;
  parameters: Record<string, unknown>;
}

// ============================================
// Abstract Provider Class
// ============================================

/**
 * Abstract base class for voice providers
 */
export abstract class VoiceProvider extends EventEmitter {
  protected config: VoiceProviderConfig;
  protected session: VoiceSession | null = null;
  protected tools: VoiceTool[] = [];

  constructor(config: VoiceProviderConfig) {
    super();
    this.config = config;
  }

  /**
   * Get provider type
   */
  abstract getType(): VoiceProviderType;

  /**
   * Get audio configuration for this provider
   */
  abstract getAudioConfig(): { input: AudioConfig; output: AudioConfig };

  /**
   * Connect and start a new session
   */
  abstract connect(userId: string): Promise<VoiceSession>;

  /**
   * Disconnect and end session
   */
  abstract disconnect(): Promise<void>;

  /**
   * Send audio chunk to the provider
   */
  abstract sendAudio(chunk: Buffer): void;

  /**
   * Commit the audio buffer (trigger response)
   */
  abstract commitAudio(): void;

  /**
   * Send a text message (instead of audio)
   */
  abstract sendText(text: string): void;

  /**
   * Interrupt current response
   */
  abstract interrupt(): void;

  /**
   * Update session configuration
   */
  abstract updateConfig(config: Partial<VoiceProviderConfig>): void;

  /**
   * Register tools for function calling
   */
  registerTools(tools: VoiceTool[]): void {
    this.tools = tools;
  }

  /**
   * Send tool result back to the model
   */
  abstract sendToolResult(callId: string, result: unknown): void;

  /**
   * Get current session
   */
  getSession(): VoiceSession | null {
    return this.session;
  }

  /**
   * Check if connected
   */
  isConnected(): boolean {
    return this.session?.status === "connected" ||
           this.session?.status === "listening" ||
           this.session?.status === "speaking";
  }

  /**
   * Get session duration in milliseconds
   */
  getDuration(): number {
    if (!this.session) return 0;
    return Date.now() - this.session.createdAt.getTime();
  }

  /**
   * Get average latency
   */
  getAverageLatency(): number {
    const latencies = this.session?.stats.latencyMs ?? [];
    if (latencies.length === 0) return 0;
    return latencies.reduce((a, b) => a + b, 0) / latencies.length;
  }

  /**
   * Create a new session object
   */
  protected createSession(userId: string): VoiceSession {
    return {
      id: `${this.getType()}-${userId}-${Date.now()}`,
      provider: this.getType(),
      userId,
      status: "connecting",
      createdAt: new Date(),
      lastActivity: new Date(),
      turnCount: 0,
      durationMs: 0,
      stats: {
        inputAudioBytes: 0,
        outputAudioBytes: 0,
        inputTokens: 0,
        outputTokens: 0,
        latencyMs: [],
        interruptions: 0,
      },
    };
  }

  /**
   * Update session status
   */
  protected updateStatus(status: SessionStatus): void {
    if (this.session) {
      this.session.status = status;
      this.session.lastActivity = new Date();
      this.session.durationMs = this.getDuration();
    }
  }
}

// ============================================
// Factory & Utilities
// ============================================

/**
 * Default configurations for each provider
 */
export const DEFAULT_PROVIDER_CONFIG: Record<VoiceProviderType, Partial<VoiceProviderConfig>> = {
  openai: {
    model: "gpt-4o-realtime-preview-2024-12-17",
    voice: "nova",
    enableVAD: true,
    vadThreshold: 0.5,
    silenceDurationMs: 500,
    maxDurationMs: 600000, // 10 minutes
  },
  gemini: {
    model: "gemini-2.5-flash-preview-native-audio-dialog",
    voice: "Puck", // Gemini voice options: Puck, Charon, Kore, Fenrir, Aoede
    enableVAD: true,
    vadThreshold: 0.5,
    silenceDurationMs: 500,
    maxDurationMs: 600000,
  },
};

/**
 * Korean-optimized voice settings
 */
export const KOREAN_VOICE_SETTINGS: Record<VoiceProviderType, Partial<VoiceProviderConfig>> = {
  openai: {
    voice: "nova", // Good Korean support
    instructions: `ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ë¹„ì„œì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”.
ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ëŒ€ë‹µí•˜ë˜, ë„ˆë¬´ ë”±ë”±í•˜ì§€ ì•Šê²Œ ì¹œê·¼í•œ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.`,
    language: "ko",
  },
  gemini: {
    voice: "Kore", // Gemini's Korean-friendly voice
    instructions: `ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ë¹„ì„œì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”.
ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ëŒ€ë‹µí•˜ë˜, ë„ˆë¬´ ë”±ë”±í•˜ì§€ ì•Šê²Œ ì¹œê·¼í•œ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.`,
    language: "ko",
  },
};

/**
 * Get provider-specific audio configuration
 */
export function getAudioConfig(provider: VoiceProviderType): { input: AudioConfig; output: AudioConfig } {
  switch (provider) {
    case "openai":
      return OPENAI_AUDIO_CONFIG;
    case "gemini":
      return GEMINI_AUDIO_CONFIG;
    default:
      throw new Error(`Unknown provider: ${provider}`);
  }
}

/**
 * Check if provider is available (API key configured)
 */
export function isProviderAvailable(provider: VoiceProviderType): boolean {
  switch (provider) {
    case "openai":
      return !!(process.env.OPENAI_API_KEY || process.env.MOLTBOT_OPENAI_API_KEY);
    case "gemini":
      return !!(process.env.GOOGLE_API_KEY || process.env.GEMINI_API_KEY || process.env.MOLTBOT_GEMINI_API_KEY);
    default:
      return false;
  }
}

/**
 * Get available providers
 */
export function getAvailableProviders(): VoiceProviderType[] {
  const providers: VoiceProviderType[] = [];
  if (isProviderAvailable("openai")) providers.push("openai");
  if (isProviderAvailable("gemini")) providers.push("gemini");
  return providers;
}

/**
 * Format provider comparison for display
 */
export function formatProviderComparison(): string {
  return `ğŸ™ï¸ **ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™” ì œê³µì**

| í•­ëª© | OpenAI Realtime | Gemini Live |
|------|-----------------|-------------|
| ëª¨ë¸ | gpt-4o-realtime | gemini-2.5-flash |
| ì…ë ¥ | PCM 24kHz | PCM 16kHz |
| ì¶œë ¥ | PCM 24kHz | PCM 24kHz |
| ì§€ì—° | ~300ms | ~200ms |
| í•œêµ­ì–´ | âœ… (nova) | âœ… (Kore) |
| ì¸í„°ëŸ½íŠ¸ | âœ… | âœ… |
| ë„êµ¬ í˜¸ì¶œ | âœ… | âœ… |
| íŠ¹ì§• | ì•ˆì •ì„± ìš°ìˆ˜ | ë„¤ì´í‹°ë¸Œ ì˜¤ë””ì˜¤ |

**ì„ íƒ ê¸°ì¤€:**
â€¢ ì•ˆì •ì„± ì¤‘ìš” â†’ OpenAI
â€¢ ìµœì € ì§€ì—° í•„ìš” â†’ Gemini
â€¢ ë¹„ìš© ì ˆê° â†’ Gemini (ë” ì €ë ´)`;
}
