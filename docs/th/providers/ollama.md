---
summary: "รันOpenClawด้วยOllama(รันไทม์LLMภายในเครื่อง)"
read_when:
  - คุณต้องการรันOpenClawด้วยโมเดลภายในเครื่องผ่านOllama
  - คุณต้องการคำแนะนำการตั้งค่าและคอนฟิกOllama
title: "Ollama"
---

# Ollama

Ollama เป็นรันไทม์ LLM แบบโลคัลที่ช่วยให้รันโมเดลโอเพนซอร์สบนเครื่องของคุณได้อย่างง่ายดาย Ollamaเป็นรันไทม์LLMภายในเครื่องที่ช่วยให้รันโมเดลโอเพนซอร์สบนเครื่องของคุณได้อย่างง่ายดายOpenClawผสานรวมกับAPIที่เข้ากันได้กับOpenAIของOllamaและสามารถ**ค้นพบโมเดลที่รองรับเครื่องมือโดยอัตโนมัติ**เมื่อคุณเลือกใช้ด้วย`OLLAMA_API_KEY`(หรือโปรไฟล์การยืนยันตัวตน)และไม่ได้กำหนดรายการ`models.providers.ollama`แบบชัดเจน

## เริ่มต้นอย่างรวดเร็ว

1. ติดตั้งOllama: [https://ollama.ai](https://ollama.ai)

2. ดึงโมเดล:

```bash
ollama pull gpt-oss:20b
# or
ollama pull llama3.3
# or
ollama pull qwen2.5-coder:32b
# or
ollama pull deepseek-r1:32b
```

3. เปิดใช้งานOllamaสำหรับOpenClaw(ตั้งค่าเป็นค่าใดก็ได้Ollamaไม่ต้องใช้คีย์จริง):

```bash
# Set environment variable
export OLLAMA_API_KEY="ollama-local"

# Or configure in your config file
openclaw config set models.providers.ollama.apiKey "ollama-local"
```

4. ใช้โมเดลจากOllama:

```json5
{
  agents: {
    defaults: {
      model: { primary: "ollama/gpt-oss:20b" },
    },
  },
}
```

## การค้นพบโมเดล(ผู้ให้บริการแบบอัตโนมัติ)

เมื่อคุณตั้งค่า`OLLAMA_API_KEY`(หรือโปรไฟล์การยืนยันตัวตน)และ**ไม่ได้**กำหนด`models.providers.ollama`OpenClawจะค้นพบโมเดลจากอินสแตนซ์Ollamaภายในเครื่องที่`http://127.0.0.1:11434`:

- เรียกดู`/api/tags`และ`/api/show`
- เก็บเฉพาะโมเดลที่รายงานความสามารถ`tools`
- ทำเครื่องหมาย`reasoning`เมื่อโมเดลรายงาน`thinking`
- อ่านค่า`contextWindow`จาก`model_info["<arch>.context_length"]`เมื่อมี
- ตั้งค่า`maxTokens`เป็น10×ของหน้าต่างบริบท
- ตั้งค่าค่าใช้จ่ายทั้งหมดเป็น`0`

วิธีนี้ช่วยหลีกเลี่ยงการเพิ่มรายการโมเดลด้วยตนเองพร้อมคงแคตตาล็อกให้สอดคล้องกับความสามารถของOllama

เพื่อดูว่ามีโมเดลใดบ้าง:

```bash
ollama list
openclaw models list
```

เพื่อเพิ่มโมเดลใหม่เพียงดึงด้วยOllama:

```bash
ollama pull mistral
```

โมเดลใหม่จะถูกค้นพบโดยอัตโนมัติและพร้อมใช้งาน

หากคุณตั้งค่า`models.providers.ollama`แบบชัดเจนการค้นพบอัตโนมัติจะถูกข้ามและคุณต้องกำหนดโมเดลด้วยตนเอง(ดูด้านล่าง)

## การกำหนดค่า

### การตั้งค่าพื้นฐาน(การค้นพบอัตโนมัติ)

วิธีที่ง่ายที่สุดในการเปิดใช้งานOllamaคือผ่านตัวแปรสภาพแวดล้อม:

```bash
export OLLAMA_API_KEY="ollama-local"
```

### การตั้งค่าแบบชัดเจน(โมเดลด้วยตนเอง)

ใช้คอนฟิกแบบชัดเจนเมื่อ:

- Ollamaทำงานบนโฮสต์หรือพอร์ตอื่น
- คุณต้องการบังคับหน้าต่างบริบทหรือรายการโมเดลเฉพาะ
- คุณต้องการรวมโมเดลที่ไม่รายงานการรองรับเครื่องมือ

```json5
{
  models: {
    providers: {
      ollama: {
        // Use a host that includes /v1 for OpenAI-compatible APIs
        baseUrl: "http://ollama-host:11434/v1",
        apiKey: "ollama-local",
        api: "openai-completions",
        models: [
          {
            id: "gpt-oss:20b",
            name: "GPT-OSS 20B",
            reasoning: false,
            input: ["text"],
            cost: { input: 0, output: 0, cacheRead: 0, cacheWrite: 0 },
            contextWindow: 8192,
            maxTokens: 8192 * 10
          }
        ]
      }
    }
  }
}
```

หากตั้งค่า`OLLAMA_API_KEY`ไว้คุณสามารถละเว้น`apiKey`ในรายการผู้ให้บริการได้และOpenClawจะเติมค่าให้สำหรับการตรวจสอบความพร้อมใช้งาน

### URLฐานแบบกำหนดเอง(คอนฟิกแบบชัดเจน)

หากOllamaทำงานบนโฮสต์หรือพอร์ตที่ต่างออกไป(คอนฟิกแบบชัดเจนจะปิดการค้นพบอัตโนมัติดังนั้นต้องกำหนดโมเดลด้วยตนเอง):

```json5
{
  models: {
    providers: {
      ollama: {
        apiKey: "ollama-local",
        baseUrl: "http://ollama-host:11434/v1",
      },
    },
  },
}
```

### การเลือกโมเดล

เมื่อกำหนดค่าแล้วโมเดลOllamaทั้งหมดของคุณจะพร้อมใช้งาน:

```json5
{
  agents: {
    defaults: {
      model: {
        primary: "ollama/gpt-oss:20b",
        fallbacks: ["ollama/llama3.3", "ollama/qwen2.5-coder:32b"],
      },
    },
  },
}
```

## ขั้นสูง

### โมเดลด้านการให้เหตุผล

OpenClawจะทำเครื่องหมายโมเดลว่าสามารถให้เหตุผลได้เมื่อOllamaรายงาน`thinking`ใน`/api/show`:

```bash
ollama pull deepseek-r1:32b
```

### ค่าใช้จ่ายของโมเดล

Ollamaใช้งานได้ฟรีและรันภายในเครื่องดังนั้นค่าใช้จ่ายของโมเดลทั้งหมดจะถูกตั้งเป็น$0

### การกำหนดค่าการสตรีม

เนื่องจาก[ปัญหาที่ทราบ](https://github.com/badlogic/pi-mono/issues/1205)ในSDKพื้นฐานเกี่ยวกับรูปแบบการตอบกลับของOllama**การสตรีมถูกปิดใช้งานเป็นค่าเริ่มต้น**สำหรับโมเดลOllamaเพื่อป้องกันการตอบกลับที่เสียหายเมื่อใช้โมเดลที่รองรับเครื่องมือ วิธีนี้ช่วยป้องกันการตอบกลับที่เสียหายเมื่อใช้โมเดลที่รองรับเครื่องมือ

เมื่อปิดการสตรีมการตอบกลับจะถูกส่งทั้งหมดในครั้งเดียว(โหมดไม่สตรีม)ซึ่งช่วยหลีกเลี่ยงปัญหาที่เดลตาเนื้อหา/การให้เหตุผลสลับกันทำให้เอาต์พุตผิดเพี้ยน

#### เปิดใช้งานการสตรีมอีกครั้ง(ขั้นสูง)

หากคุณต้องการเปิดการสตรีมอีกครั้งสำหรับOllama(อาจก่อให้เกิดปัญหากับโมเดลที่รองรับเครื่องมือ):

```json5
{
  agents: {
    defaults: {
      models: {
        "ollama/gpt-oss:20b": {
          streaming: true,
        },
      },
    },
  },
}
```

#### ปิดการสตรีมสำหรับผู้ให้บริการอื่น

คุณยังสามารถปิดการสตรีมสำหรับผู้ให้บริการใดๆได้หากจำเป็น:

```json5
{
  agents: {
    defaults: {
      models: {
        "openai/gpt-4": {
          streaming: false,
        },
      },
    },
  },
}
```

### หน้าต่างบริบท

สำหรับโมเดลที่ค้นพบอัตโนมัติOpenClawจะใช้หน้าต่างบริบทที่Ollamaรายงานเมื่อมีไม่เช่นนั้นจะใช้ค่าเริ่มต้นเป็น`8192`คุณสามารถแทนที่`contextWindow`และ`maxTokens`ในคอนฟิกผู้ให้บริการแบบชัดเจนได้ คุณสามารถแทนที่ค่า `contextWindow` และ `maxTokens` ได้ในการกำหนดค่าผู้ให้บริการแบบระบุชัดเจน

## การแก้ไขปัญหา

### ไม่ตรวจพบOllama

ตรวจสอบให้แน่ใจว่าOllamaกำลังทำงานและคุณได้ตั้งค่า`OLLAMA_API_KEY`(หรือโปรไฟล์การยืนยันตัวตน)และคุณ**ไม่ได้**กำหนดรายการ`models.providers.ollama`แบบชัดเจน:

```bash
ollama serve
```

และAPIสามารถเข้าถึงได้:

```bash
curl http://localhost:11434/api/tags
```

### ไม่มีโมเดลให้ใช้งาน

OpenClawจะค้นพบอัตโนมัติเฉพาะโมเดลที่รายงานการรองรับเครื่องมือหากโมเดลของคุณไม่แสดงในรายการให้ทำอย่างใดอย่างหนึ่ง: หากโมเดลของคุณไม่อยู่ในรายการ ให้เลือกอย่างใดอย่างหนึ่ง:

- ดึงโมเดลที่รองรับเครื่องมือหรือ
- กำหนดโมเดลแบบชัดเจนใน`models.providers.ollama`

เพื่อเพิ่มโมเดล:

```bash
ollama list  # See what's installed
ollama pull gpt-oss:20b  # Pull a tool-capable model
ollama pull llama3.3     # Or another model
```

### การเชื่อมต่อถูกปฏิเสธ

ตรวจสอบว่าOllamaกำลังทำงานบนพอร์ตที่ถูกต้อง:

```bash
# Check if Ollama is running
ps aux | grep ollama

# Or restart Ollama
ollama serve
```

### การตอบกลับเสียหายหรือมีชื่อเครื่องมือในเอาต์พุต

หากคุณเห็นการตอบกลับที่ผิดเพี้ยนซึ่งมีชื่อเครื่องมือ(เช่น`sessions_send`, `memory_get`)หรือข้อความแตกเป็นส่วนๆเมื่อใช้โมเดลOllamaสาเหตุเกิดจากปัญหาในSDKต้นทางกับการตอบกลับแบบสตรีม**ปัญหานี้ได้รับการแก้ไขเป็นค่าเริ่มต้น**ในOpenClawเวอร์ชันล่าสุดโดยการปิดการสตรีมสำหรับโมเดลOllama **สิ่งนี้ถูกแก้ไขโดยค่าเริ่มต้นแล้ว** ใน OpenClaw เวอร์ชันล่าสุด โดยปิดการสตรีมสำหรับโมเดล Ollama

หากคุณเปิดการสตรีมด้วยตนเองและพบปัญหานี้:

1. ลบการกำหนดค่า`streaming: true`ออกจากรายการโมเดลOllamaของคุณหรือ
2. ตั้งค่า`streaming: false`สำหรับโมเดลOllamaแบบชัดเจน(ดู[การกำหนดค่าการสตรีม](#streaming-configuration))

## ดูเพิ่มเติม

- [ผู้ให้บริการโมเดล](/concepts/model-providers) - ภาพรวมของผู้ให้บริการทั้งหมด
- [การเลือกโมเดล](/concepts/models) - วิธีเลือกโมเดล
- [การกำหนดค่า](/gateway/configuration) - เอกสารอ้างอิงคอนฟิกทั้งหมด
