---
summary: "‚ÄûEingehende Bild-/Audio-/Video-Erkennung (optional) mit Anbieter- und CLI-Fallbacks‚Äú"
read_when:
  - Entwurf oder Refactoring der Medienerkennung
  - Feinabstimmung der eingehenden Audio-/Video-/Bildvorverarbeitung
title: "Medienverst√§ndnis"
---

# Medienerkennung (Eingehend) ‚Äî 2026-01-17

OpenClaw kann **eingehende Medien zusammenfassen** (Bild/Audio/Video), bevor die Antwort-Pipeline l√§uft. Es erkennt automatisch, ob lokale Werkzeuge oder Anbieter-Schl√ºssel verf√ºgbar sind, und kann deaktiviert oder angepasst werden. Ist die Erkennung ausgeschaltet, erhalten Modelle weiterhin wie gewohnt die Originaldateien/URLs.

## Ziele

- Optional: Vorab-Aufbereitung eingehender Medien zu kurzem Text f√ºr schnelleres Routing und bessere Befehlsauswertung.
- Originale Medienweitergabe an das Modell beibehalten (immer).
- Unterst√ºtzung von **Anbieter-APIs** und **CLI-Fallbacks**.
- Mehrere Modelle mit geordnetem Fallback (Fehler/Gr√∂√üe/Timeout) erlauben.

## Verhalten auf hoher Ebene

1. Sammeln eingehender Anh√§nge (`MediaPaths`, `MediaUrls`, `MediaTypes`).
2. F√ºr jede aktivierte F√§higkeit (Bild/Audio/Video) Anh√§nge gem√§√ü Richtlinie ausw√§hlen (Standard: **first**).
3. Den ersten geeigneten Modelletrag w√§hlen (Gr√∂√üe + F√§higkeit + Auth).
4. Wenn ein Modell fehlschl√§gt oder das Medium zu gro√ü ist, **auf den n√§chsten Eintrag zur√ºckfallen**.
5. Bei Erfolg:
   - `Body` wird zu einem `[Image]`-, `[Audio]`- oder `[Video]`-Block.
   - Audio setzt `{{Transcript}}`; die Befehlsauswertung verwendet den Caption-Text, sofern vorhanden,
     andernfalls das Transkript.
   - Captions werden als `User text:` innerhalb des Blocks beibehalten.

Wenn die Erkennung fehlschl√§gt oder deaktiviert ist, **l√§uft der Antwortfluss weiter** mit dem Originaltext + Anh√§ngen.

## Konfigurations√ºbersicht

`tools.media` unterst√ºtzt **gemeinsame Modelle** sowie √úberschreibungen pro F√§higkeit:

- `tools.media.models`: gemeinsame Modellliste (mit `capabilities` steuern).
- `tools.media.image` / `tools.media.audio` / `tools.media.video`:
  - Standards (`prompt`, `maxChars`, `maxBytes`, `timeoutSeconds`, `language`)
  - Anbieter-√úberschreibungen (`baseUrl`, `headers`, `providerOptions`)
  - Deepgram-Audiooptionen √ºber `tools.media.audio.providerOptions.deepgram`
  - optionale **f√§higkeitsbezogene `models`-Liste** (bevorzugt vor gemeinsamen Modellen)
  - `attachments`-Richtlinie (`mode`, `maxAttachments`, `prefer`)
  - `scope` (optional: Steuerung nach Kanal/Chat-Typ/Sitzungsschl√ºssel)
- `tools.media.concurrency`: maximale gleichzeitige F√§higkeitsl√§ufe (Standard **2**).

```json5
{
  tools: {
    media: {
      models: [
        /* shared list */
      ],
      image: {
        /* optional overrides */
      },
      audio: {
        /* optional overrides */
      },
      video: {
        /* optional overrides */
      },
    },
  },
}
```

### Modelletr√§ge

Jeder `models[]`-Eintrag kann **Anbieter** oder **CLI** sein:

```json5
{
  type: "provider", // default if omitted
  provider: "openai",
  model: "gpt-5.2",
  prompt: "Describe the image in <= 500 chars.",
  maxChars: 500,
  maxBytes: 10485760,
  timeoutSeconds: 60,
  capabilities: ["image"], // optional, used for multi‚Äëmodal entries
  profile: "vision-profile",
  preferredProfile: "vision-fallback",
}
```

```json5
{
  type: "cli",
  command: "gemini",
  args: [
    "-m",
    "gemini-3-flash",
    "--allowed-tools",
    "read_file",
    "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters.",
  ],
  maxChars: 500,
  maxBytes: 52428800,
  timeoutSeconds: 120,
  capabilities: ["video", "image"],
}
```

CLI-Vorlagen k√∂nnen au√üerdem verwenden:

- `{{MediaDir}}` (Verzeichnis, das die Mediendatei enth√§lt)
- `{{OutputDir}}` (f√ºr diesen Lauf erstelltes Scratch-Verzeichnis)
- `{{OutputBase}}` (Basis-Pfad der Scratch-Datei, ohne Erweiterung)

## Standards und Limits

Empfohlene Standards:

- `maxChars`: **500** f√ºr Bild/Video (kurz, befehlsfreundlich)
- `maxChars`: **nicht gesetzt** f√ºr Audio (vollst√§ndiges Transkript, sofern kein Limit gesetzt ist)
- `maxBytes`:
  - Bild: **10MB**
  - Audio: **20MB**
  - Video: **50MB**

Regeln:

- √úberschreitet ein Medium `maxBytes`, wird dieses Modell √ºbersprungen und **das n√§chste Modell wird versucht**.
- Gibt das Modell mehr als `maxChars` zur√ºck, wird die Ausgabe gek√ºrzt.
- `prompt` ist standardm√§√üig ein einfaches ‚ÄûDescribe the {media}.‚Äú plus die `maxChars`-Leitlinie (nur Bild/Video).
- Wenn `<capability>.enabled: true`, aber keine Modelle konfiguriert sind, versucht OpenClaw das
  **aktive Antwortmodell**, sofern dessen Anbieter die F√§higkeit unterst√ºtzt.

### Automatische Erkennung der Medienerkennung (Standard)

Wenn `tools.media.<capability>.enabled` **nicht** auf `false` gesetzt ist und Sie keine
Modelle konfiguriert haben, erkennt OpenClaw automatisch in dieser Reihenfolge und **stoppt bei der ersten
funktionierenden Option**:

1. **Lokale CLIs** (nur Audio; falls installiert)
   - `sherpa-onnx-offline` (erfordert `SHERPA_ONNX_MODEL_DIR` mit Encoder/Decoder/Joiner/Tokens)
   - `whisper-cli` (`whisper-cpp`; verwendet `WHISPER_CPP_MODEL` oder das geb√ºndelte Tiny-Modell)
   - `whisper` (Python-CLI; l√§dt Modelle automatisch herunter)
2. **Gemini CLI** (`gemini`) mit `read_many_files`
3. **Anbieter-Schl√ºssel**
   - Audio: OpenAI ‚Üí Groq ‚Üí Deepgram ‚Üí Google
   - Bild: OpenAI ‚Üí Anthropic ‚Üí Google ‚Üí MiniMax
   - Video: Google

Um die automatische Erkennung zu deaktivieren, setzen Sie:

```json5
{
  tools: {
    media: {
      audio: {
        enabled: false,
      },
    },
  },
}
```

Hinweis: Die Bin√§rerkennung ist bestm√∂glich √ºber macOS/Linux/Windows hinweg; stellen Sie sicher, dass die CLI auf `PATH` liegt (wir erweitern `~`), oder setzen Sie ein explizites CLI-Modell mit vollst√§ndigem Befehls-Pfad.

## F√§higkeiten (optional)

Wenn Sie `capabilities` setzen, l√§uft der Eintrag nur f√ºr diese Medientypen. F√ºr gemeinsame
Listen kann OpenClaw Standardwerte ableiten:

- `openai`, `anthropic`, `minimax`: **image**
- `google` (Gemini API): **image + audio + video**
- `groq`: **audio**
- `deepgram`: **audio**

F√ºr CLI-Eintr√§ge **setzen Sie `capabilities` explizit**, um √ºberraschende Zuordnungen zu vermeiden.
Wenn Sie `capabilities` weglassen, ist der Eintrag f√ºr die Liste geeignet, in der er erscheint.

## Anbieter-Unterst√ºtzungsmatrix (OpenClaw-Integrationen)

| F√§higkeit | Anbieter-Integration                              | Hinweise                                                                             |
| --------- | ------------------------------------------------- | ------------------------------------------------------------------------------------ |
| Bild      | OpenAI / Anthropic / Google / andere √ºber `pi-ai` | Jedes bildf√§hige Modell im Register funktioniert.                    |
| Audio     | OpenAI, Groq, Deepgram, Google                    | Anbieter-Transkription (Whisper/Deepgram/Gemini). |
| Video     | Google (Gemini API)            | Anbieter-Videoerkennung.                                             |

## Empfohlene Anbieter

**Bild**

- Bevorzugen Sie Ihr aktives Modell, wenn es Bilder unterst√ºtzt.
- Gute Standards: `openai/gpt-5.2`, `anthropic/claude-opus-4-6`, `google/gemini-3-pro-preview`.

**Audio**

- `openai/gpt-4o-mini-transcribe`, `groq/whisper-large-v3-turbo` oder `deepgram/nova-3`.
- CLI-Fallback: `whisper-cli` (whisper-cpp) oder `whisper`.
- Deepgram-Einrichtung: [Deepgram (Audiotranskription)](/providers/deepgram).

**Video**

- `google/gemini-3-flash-preview` (schnell), `google/gemini-3-pro-preview` (umfangreicher).
- CLI-Fallback: `gemini`-CLI (unterst√ºtzt `read_file` f√ºr Video/Audio).

## Anhang-Richtlinie

Die f√§higkeitsbezogene `attachments` steuert, welche Anh√§nge verarbeitet werden:

- `mode`: `first` (Standard) oder `all`
- `maxAttachments`: Begrenzung der verarbeiteten Anzahl (Standard **1**)
- `prefer`: `first`, `last`, `path`, `url`

Wenn `mode: "all"`, werden Ausgaben als `[Image 1/2]`, `[Audio 2/2]` usw.

## Konfigurationsbeispiele

### 1. Gemeinsame Modellliste + √úberschreibungen

```json5
{
  tools: {
    media: {
      models: [
        { provider: "openai", model: "gpt-5.2", capabilities: ["image"] },
        {
          provider: "google",
          model: "gemini-3-flash-preview",
          capabilities: ["image", "audio", "video"],
        },
        {
          type: "cli",
          command: "gemini",
          args: [
            "-m",
            "gemini-3-flash",
            "--allowed-tools",
            "read_file",
            "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters.",
          ],
          capabilities: ["image", "video"],
        },
      ],
      audio: {
        attachments: { mode: "all", maxAttachments: 2 },
      },
      video: {
        maxChars: 500,
      },
    },
  },
}
```

### 2. Nur Audio + Video (Bild aus)

```json5
{
  tools: {
    media: {
      audio: {
        enabled: true,
        models: [
          { provider: "openai", model: "gpt-4o-mini-transcribe" },
          {
            type: "cli",
            command: "whisper",
            args: ["--model", "base", "{{MediaPath}}"],
          },
        ],
      },
      video: {
        enabled: true,
        maxChars: 500,
        models: [
          { provider: "google", model: "gemini-3-flash-preview" },
          {
            type: "cli",
            command: "gemini",
            args: [
              "-m",
              "gemini-3-flash",
              "--allowed-tools",
              "read_file",
              "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters.",
            ],
          },
        ],
      },
    },
  },
}
```

### 3. Optionale Bilderkennung

```json5
{
  tools: {
    media: {
      image: {
        enabled: true,
        maxBytes: 10485760,
        maxChars: 500,
        models: [
          { provider: "openai", model: "gpt-5.2" },
          { provider: "anthropic", model: "claude-opus-4-6" },
          {
            type: "cli",
            command: "gemini",
            args: [
              "-m",
              "gemini-3-flash",
              "--allowed-tools",
              "read_file",
              "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters.",
            ],
          },
        ],
      },
    },
  },
}
```

### 4. Multimodaler Einzeleintrag (explizite F√§higkeiten)

```json5
{
  tools: {
    media: {
      image: {
        models: [
          {
            provider: "google",
            model: "gemini-3-pro-preview",
            capabilities: ["image", "video", "audio"],
          },
        ],
      },
      audio: {
        models: [
          {
            provider: "google",
            model: "gemini-3-pro-preview",
            capabilities: ["image", "video", "audio"],
          },
        ],
      },
      video: {
        models: [
          {
            provider: "google",
            model: "gemini-3-pro-preview",
            capabilities: ["image", "video", "audio"],
          },
        ],
      },
    },
  },
}
```

## Statusausgabe

Wenn die Medienerkennung l√§uft, enth√§lt `/status` eine kurze Zusammenfassungszeile:

```
üìé Media: image ok (openai/gpt-5.2) ¬∑ audio skipped (maxBytes)
```

Diese zeigt pro F√§higkeit die Ergebnisse sowie den gew√§hlten Anbieter/das Modell, falls zutreffend.

## Hinweise

- Die Erkennung ist **Best-Effort**. Fehler blockieren Antworten nicht.
- Anh√§nge werden auch dann an Modelle weitergereicht, wenn die Erkennung deaktiviert ist.
- Verwenden Sie `scope`, um einzuschr√§nken, wo die Erkennung l√§uft (z.‚ÄØB.

## Verwandte Dokumente

- [Konfiguration](/gateway/configuration)
- [Bild- & Medienunterst√ºtzung](/nodes/images)
